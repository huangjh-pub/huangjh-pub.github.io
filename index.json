[{"authors":["jiahui-huang"],"categories":null,"content":"I am currently a research scientist at NVIDIA Toronto AI Lab led by Sanja Fidler. I received my Ph.D. in 2023 from the Graphics and Geometric Computing Group at Tsinghua University, China, advised by Shi-Min Hu. I was also a visiting researcher in the Geometric Computing group at Stanford University, led by Leonidas Guibas. My primary research interest lies in the joint field of 3D computer vision and graphics, including neural reconstruction, dynamic scene perception, and SLAM.\n⛵️ We are actively looking for interns to work with at NVIDIA Research. Please reach out to me if you are interested in the position.\nThis page was last updated in Aug 2023.\n","date":1690956932,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690956932,"objectID":"680d39ddfa64e25f048e46771f64b145","permalink":"https://huangjh-pub.github.io/authors/jiahui-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiahui-huang/","section":"authors","summary":"I am currently a research scientist at NVIDIA Toronto AI Lab led by Sanja Fidler. I received my Ph.D. in 2023 from the Graphics and Geometric Computing Group at Tsinghua University, China, advised by Shi-Min Hu.","tags":null,"title":"Jiahui Huang","type":"authors"},{"authors":["Kiyohiro Nakayama","Mikaela Angelina Uy","Jiahui Huang","Shi-Min Hu","Ke Li","Leonidas J. Guibas"],"categories":[],"content":"","date":1690956932,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690956932,"objectID":"645cf0c8808dc3afa65a6e0262bcaf09","permalink":"https://huangjh-pub.github.io/publication/diffacto/","publishdate":"2023-08-01T23:15:32-07:00","relpermalink":"/publication/diffacto/","section":"publication","summary":"International Conference on Computer Vision (ICCV) 2023","tags":[],"title":"DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion","type":"publication"},{"authors":["Jiahui Huang","Zan Gojcic","Matan Atzmon","Or Litany","Sanja Fidler","Francis Williams"],"categories":[],"content":" Kernel Surface Reconstruction (NKSR) recovers a 3D surface from an input point cloud.Trained directly from dense points, our method reaches state-of-the-art reconstruction quality and scalability. All the meshes in this figure are reconstructed using a single trained model. We present a novel method for reconstructing a 3D implicit surface from a large-scale, sparse, and noisy point cloud. Our approach builds upon the recently introduced Neural Kernel Fields (NKF) representation. It enjoys similar generalization capabilities to NKF, while simultaneously addressing its main limitations: (a) We can scale to large scenes through compactly supported kernel functions, which enable the use of memory-efficient sparse linear solvers. (b) We are robust to noise, through a gradient fitting solve. (c) We minimize training requirements, enabling us to learn from any dataset of dense oriented points, and even mix training data consisting of objects and scenes at different scales. Our method is capable of reconstructing millions of points in a few seconds, and handling very large scenes in an out-of-core fashion. We achieve state-of-the-art results on reconstruction benchmarks consisting of single objects, indoor scenes, and outdoor scenes.\nMethod Our method accepts an oriented point cloud and predicts a sparse hierarchy of voxel grids containing features as well as normals in each voxel. We then construct a sparse linear system and solve for a set of per-voxel coefficients $\\alpha$. The linear system corresponds to the gram matrix arising from a kernel which depends on the predicted features, illustrated as $\\mathbf{L}$ and $v$ above (described in the paper). To extract the predicted surface, we evaluate the function values at the voxel corners using a linear combination of the learned kernel basis functions, followed by dual marching cubes. Watch the explanatory video to gain more intuition of our method design.\nQualitative Resutls NKSR is accurate, generalizable and scalable. These three main properties are demonstrated using the following three types of datasets.\nSingle Objects InputPOCONeural Kernel FieldNeural GalerkinOurs (NKSR) Indoor Scenes All the methods below are trained only on ShapeNet dataset and are directly tested on ScanNet and Matterport3D datasets. Hover over each individual image to visualize the normal map.\nLocal Implicit Grid Neural Kernel Field POCO Ours (NKSR) Local Implicit Grid Dual Octree GNN POCO Ours (NKSR) Driving Scenes We synthesize the first dataset for benchmarking large-scale surface reconstruction using the CARLA simulator.​ We accumulate the LiDAR points from the sensor and crop the geometries into 51.2m × 51.2m chunks. Move the slider to see side-by-side comparisons with the baselines (refresh your browser if the videos do not align).\nCARLA origin subset:\nCARLA novel subset:\nThe following shows direct generalization results to Waymo Open Dataset. During training time the model has never seen real outdoor AV data.​ Click to zoom in.\nExplanatory Video Citation If you find our work interesting, please consider citing us:\n@inproceedings{huang2023nksr, title={Neural Kernel Surface Reconstruction}, author={Huang, Jiahui and Gojcic, Zan and Atzmon, Matan and Litany, Or and Fidler, Sanja and Williams, Francis}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={4369--4379}, year={2023} } ","date":1680357580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680357580,"objectID":"efbb716f09a48a0ae21c4ef62bd6e58f","permalink":"https://huangjh-pub.github.io/publication/nksr/","publishdate":"2023-04-01T21:59:40+08:00","relpermalink":"/publication/nksr/","section":"publication","summary":"Computer Vision and Pattern Recognition (CVPR) 2023 - **Highlight**","tags":[],"title":"Neural Kernel Surface Reconstruction","type":"publication"},{"authors":["Jiahui Huang","Hao-Xiang Chen","Shi-Min Hu"],"categories":[],"content":"","date":1670335180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670335180,"objectID":"4df1b38505d03014464dba85479b91b1","permalink":"https://huangjh-pub.github.io/publication/neuralgalerkin/","publishdate":"2022-12-06T21:59:40+08:00","relpermalink":"/publication/neuralgalerkin/","section":"publication","summary":"SIGGRAPH Asia 2022","tags":[],"title":"A Neural Galerkin Solver for Accurate Surface Reconstruction","type":"publication"},{"authors":["Jiahui Huang","Tolga Birdal","Zan Gojcic","Leonidas J. Guibas","Shi-Min Hu"],"categories":[],"content":" We present SyNoRiM, a novel way to jointly register Multiple Non-Rigid shapes by Synchronizing the maps that relate learned functions defined on the point clouds. Even though the ability to process non-rigid shapes is critical in various applications ranging from computer animation to 3D digitization, the literature still lacks a robust and flexible framework to match and align a collection of real, noisy scans observed under occlusions. Given a set of such point clouds, our method first computes the pairwise correspondences parameterized via functional maps. We simultaneously learn potentially non-orthogonal basis functions to effectively regularize the deformations, while handling the occlusions in an elegant way. To maximally benefit from the multi-way information provided by the inferred pairwise deformation fields, we synchronize the pairwise functional maps into a cycle-consistent whole thanks to our novel and principled optimization formulation. We demonstrate via extensive experiments that our method achieves a state-of-the-art performance in registration accuracy, while being flexible and efficient as we handle both non-rigid and multi-body cases in a unified framework and avoid the costly optimization over point-wise permutations by the use of basis function maps.\nMethod The input to our method is a set of point clouds $\\{ \\mathbf{X}_k \\in \\mathbb{R}^{N_k \\times 3} \\}$ and the output of our method contains all the pairwise per-point 3D flow vectors $\\{ \\mathbf{F}_{kl} \\in \\mathbb{R}^{N_k \\times 3} \\}$. The flow vectors naturally induce the non-rigid warp field from $\\mathbf{X}_k$ to $\\mathbf{X}_l$ as $ \\mathcal{W}_k (\\mathbf{X}_k) = \\mathbf{X}_k + \\mathbf{F}_{kl} $, optimally aligning the given point cloud pairs by deforming the source $\\mathbf{X}_k$ onto the target $\\mathbf{X}_l$. For multiple input clouds we additionally encourage the cyclic consistency of the flow estimates, i.e.: $ \\mathcal{W}_{k_1} \\circ \\mathcal{W}_{k_2} \\circ ... \\circ \\mathcal{W}_{k_p} \\approx \\mathcal{I} $ for all cycles in the densely-connected graph formed by the input set.\nDuring training, our method is supervised in a pairwise fashion. We first ① use a sparse-convolution-based neural network $\\varphi_{\\mathrm{desc}}$ (i.e., $\\mathfrak{g}_\\mathrm{pd}$) to establish putative correspondences between each point cloud pair. We then ② estimate a set of basis functions $\\{ \\mathbf{\\Phi}_k \\in \\mathbb{R}^{N_k \\times M} \\}$ for each point cloud using another network $\\varphi_{\\mathrm{basis}}$. By jointly modeling the correspondences and the bases in a robust way, the initial functional map $\\mathbf{C}_{kl}^0$ is obtained before being refined with $\\varphi_{\\mathrm{refine}}$ that yields pairwise flow estimates. During test time with multiple inputs, we estimate the map set $\\{\\mathbf{C}_{kl}^0\\}$ for all pairs. ③ The maps are subsequently synchronized to optimize for cycle consistency among the inputs. Finally, ④ 3D flows are estimated from the optimized functional maps $\\{\\mathbf{C}_{kl}^\\star\\}$ as our final output, using the same procedure as done in the pairwise setting. The registered point cloud is a fusion of all initial point clouds warped by the estimated flows. For more details, please read our technical report.\nResults Some demonstrative registration results are shown below, all rendered as raw point clouds. For each animation the points from all other sources are warped to the current view as target. We use datasets from CAPE, DeformingThings4D, DeepDeform, and SAPIEN.\nDynamic reconstruction result. The RGB images are just for references. We also provide extensive quantitative experiments and ablations in our main paper. Our results on FAUST online challenge can also be viewed here.\nDataset Our dataset is heavily based on published datasets. Although our modification is license-free, the original data providers may require you to accept different terms \u0026amp; conditions before being allowed to use them. Be sure to double-check that before downloading. If you find any legal issues, please let us know immediately.\n# Train # Validation # Test Origin Terms of Use Download Link MPC-CAPE 3015 798 209 CAPE Link Data (11.2G) MPC-DT4D 3907 1701 1299 DeformingThings4D Link Data (20.6G) MPC-DD 1754 200 267 DeepDeform Link Data (2.4G) MPC-SAPIEN 530 88 266 SAPIEN Link Data (1.3G) Citation If you find our work interesting, please consider citing us:\n@article{huang2021multiway, title={Multiway Non-rigid Point Cloud Registration via Learned Functional Map Synchronization}, author={Huang, Jiahui and Birdal, Tolga and Gojcic, Zan and Guibas, Leonidas J and Hu, Shi-Min}, journal={arXiv preprint arXiv:2111.12878}, year={2021} } Acknowledgements We thank all the reviewers for their thoughtful comments and constructive suggestions. This paper was supported by National Key R\u0026amp;D Program of China (project No. 2021ZD0112902), a Vannevar Bush faculty fellowship, ARL grant W911NF2120104, NSF grant IIS-1763268, and a gift from the Autodesk …","date":1638367180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638367180,"objectID":"494bdeb85243032241ff3022f4118d3d","permalink":"https://huangjh-pub.github.io/publication/synorim/","publishdate":"2021-12-01T21:59:40+08:00","relpermalink":"/publication/synorim/","section":"publication","summary":"T-PAMI 2022","tags":[],"title":"Multiway Non-rigid Point Cloud Registration via Learned Functional Map Synchronization","type":"publication"},{"authors":["Haoxiang Chen","Jiahui Huang","Tai-Jiang Mu","Shi-Min Hu"],"categories":[],"content":"","date":1635775180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635775180,"objectID":"59fbb3b796256578044ce9895be0dd93","permalink":"https://huangjh-pub.github.io/publication/circle/","publishdate":"2021-11-01T21:59:40+08:00","relpermalink":"/publication/circle/","section":"publication","summary":"ArXiv 2021","tags":[],"title":"CIRCLE: Convolutional Implicit Reconstruction and Completion for Large-scale Indoor Scene","type":"publication"},{"authors":["Shi-Sheng Huang","Haoxiang Chen","Jiahui Huang","Hongbo Fu","Shi-Min Hu"],"categories":[],"content":"","date":1635775180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635775180,"objectID":"668d2bf952f058ad8285ac4d1f536e97","permalink":"https://huangjh-pub.github.io/publication/semanticfusion/","publishdate":"2021-11-01T21:59:40+08:00","relpermalink":"/publication/semanticfusion/","section":"publication","summary":"IEEE Transactions on Visualization and Computer Graphics 2021","tags":[],"title":"Real-Time Globally Consistent 3D Reconstruction with Semantic Priors","type":"publication"},{"authors":["Shi-Min Hu","Zheng-Ning Liu","Meng-Hao Guo","Jun-Xiong Cai","Jiahui Huang","Tai-Jiang Mu","Ralph R. Martin"],"categories":[],"content":"","date":1622555980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622555980,"objectID":"67ce65a5d5130f1e2d2cfda5a588d86a","permalink":"https://huangjh-pub.github.io/publication/subdivnet/","publishdate":"2021-06-01T21:59:40+08:00","relpermalink":"/publication/subdivnet/","section":"publication","summary":"ACM Transactions on Graphics 2021","tags":[],"title":"Subdivision-Based Mesh Convolution Networks","type":"publication"},{"authors":["Jiahui Huang","He Wang","Tolga Birdal","Minhyuk Sung","Federica Arrigoni","Shi-Min Hu","Leonidas J. Guibas"],"categories":[],"content":"","date":1614607180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614607180,"objectID":"6eae6af977b487a24fa3d7b2a8ef7977","permalink":"https://huangjh-pub.github.io/publication/multibodysync/","publishdate":"2021-03-01T21:59:40+08:00","relpermalink":"/publication/multibodysync/","section":"publication","summary":"Computer Vision and Pattern Recognition (CVPR) 2021 - **Oral**","tags":[],"title":"MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization","type":"publication"},{"authors":["Jiahui Huang","Shi-Sheng Huang","Haoxuan Song","Shi-Min Hu"],"categories":[],"content":"","date":1607522467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607522467,"objectID":"c44a56fea7799c68b55e75825635cd58","permalink":"https://huangjh-pub.github.io/publication/difusion/","publishdate":"2020-03-01T22:01:07+08:00","relpermalink":"/publication/difusion/","section":"publication","summary":"Computer Vision and Pattern Recognition (CVPR) 2021","tags":[],"title":"DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors","type":"publication"},{"authors":["Jiahui Huang","Zheng-Fei Kuang","Fang-Lue Zhang","Tai-Jiang Mu"],"categories":[],"content":"","date":1598968898,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598968898,"objectID":"a45e06873a844b280d9fc17913505418","permalink":"https://huangjh-pub.github.io/publication/wallnet/","publishdate":"2020-12-09T22:01:38+08:00","relpermalink":"/publication/wallnet/","section":"publication","summary":"Graphical Models 2020","tags":[],"title":"WallNet: Reconstructing General Room Layouts from RGB Images","type":"publication"},{"authors":[],"categories":[],"content":"","date":1585890919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585890919,"objectID":"81396b6085d73f62cc2ce576fe2b182e","permalink":"https://huangjh-pub.github.io/project/3sweep-dummy/","publishdate":"2020-04-02T22:15:19-07:00","relpermalink":"/project/3sweep-dummy/","section":"project","summary":"3-Sweep is a creative method for 3D modeling from a single image, which is proposed in a SIGGRAPH ASIA 2013 paper by Tsinghua University, Tel Aviv University and The Interdisciplinary Center. According to many readers' requests, I re-implemented and released this software.","tags":[],"title":"3-Sweep: Extracting Editable Objects from a Single Photo","type":"project"},{"authors":["Jiahui Huang","Sheng Yang","Tai-Jiang Mu","Shi-Min Hu"],"categories":[],"content":"","date":1583129732,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583129732,"objectID":"340bd830d185bbe1775f2c0a8966d04a","permalink":"https://huangjh-pub.github.io/publication/clustervo/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/clustervo/","section":"publication","summary":"Computer Vision and Pattern Recognition (CVPR) 2020","tags":[],"title":"ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings","type":"publication"},{"authors":["Yinyu Nie","Shihui Guo","Jian Chang","Xiaoguang Han","Jiahui Huang","Shi-Min Hu","Jian Jun Zhang"],"categories":[],"content":"","date":1577945732,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577945732,"objectID":"919e5eb0a9bf62560adf09bbf69ff29f","permalink":"https://huangjh-pub.github.io/publication/shallow2deep/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/shallow2deep/","section":"publication","summary":"Pattern Recognition 2020","tags":[],"title":"Shallow2Deep: Indoor Scene Modeling by Single Image Understanding","type":"publication"},{"authors":["Jiahui Huang","Sheng Yang","Zishuo Zhao","Yu-Kun Lai","Shi-Min Hu"],"categories":[],"content":"","date":1569996932,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569996932,"objectID":"d9d67dabec7d157b8aa28bc7efca7289","permalink":"https://huangjh-pub.github.io/publication/clusterslam/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/clusterslam/","section":"publication","summary":"International Conference on Computer Vision (ICCV) 2019","tags":[],"title":"ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation","type":"publication"},{"authors":["Congyue Deng","Jiahui Huang","Yong-Liang Yang"],"categories":[],"content":"","date":1559456132,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559456132,"objectID":"d0b648a49077469ff92e1fe79d5f4a2e","permalink":"https://huangjh-pub.github.io/publication/lofted/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/lofted/","section":"publication","summary":"Computational Visual Media (CVM) 2019","tags":[],"title":"Interactive Modeling of Lofted Shapes from a Single Image","type":"publication"},{"authors":["Jun Gao","Chengcheng Tang","Vignesh Ganapathi-Subramanian","Jiahui Huang","Hao Su","Leonidas J. Guibas"],"categories":[],"content":"","date":1547128867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547128867,"objectID":"1b5edacfb0ef5a0f6383ffa6b3f21d49","permalink":"https://huangjh-pub.github.io/publication/deepspline/","publishdate":"2019-01-10T22:01:07+08:00","relpermalink":"/publication/deepspline/","section":"publication","summary":"ArXiv 2019","tags":[],"title":"DeepSpline: Data-Driven Reconstruction of Parametric Curves and Surfaces","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"0b942da2f81781623a815f9002373e76","permalink":"https://huangjh-pub.github.io/project/3sweep/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/3sweep/","section":"project","summary":"3-Sweep Software Download","tags":null,"title":"3-Sweep","type":"widget_page"},{"authors":["Jiahui Huang","Jun Gao","Vignesh Ganapathi-Subramanian","Hao Su","Yin Liu","Chengcheng Tang","Leonidas J. Guibas"],"categories":[],"content":"","date":1545632132,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545632132,"objectID":"865f75679318fea8d531a7eecc70fc35","permalink":"https://huangjh-pub.github.io/publication/deepprimitive/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/deepprimitive/","section":"publication","summary":"Computational Visual Media (CVM) 2018","tags":[],"title":"DeepPrimitive: Image decomposition by layered primitive detection","type":"publication"},{"authors":["Bo Ren","Jiahui Huang","Ming C. Lin","Shi-Min Hu"],"categories":[],"content":"","date":1527056132,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527056132,"objectID":"6e89502d34b590b821564bb93d64b951","permalink":"https://huangjh-pub.github.io/publication/crystal/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/crystal/","section":"publication","summary":"Eurographics (EG) 2018","tags":[],"title":"Controllable Dendritic Crystal Simulation Using Orientation Field","type":"publication"},{"authors":["Wen Hu","Jiahui Huang","Zhi Wang","Peng Wang","Kun Yi","Yonggang Wen","Kaiyan Chu","Lifeng Sun"],"categories":[],"content":"","date":1493705732,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493705732,"objectID":"12d7a8dfbc57221828f22c07879ce86d","permalink":"https://huangjh-pub.github.io/publication/musa/","publishdate":"2020-04-02T23:15:32-07:00","relpermalink":"/publication/musa/","section":"publication","summary":"International Symposium on Quality of Service (IWQoS) 2017","tags":[],"title":"MUSA: Wi-Fi AP-assisted video prefetching via Tensor Learning","type":"publication"},{"authors":null,"categories":[],"content":" Overview of the synthetic dataset Introduction ClusterSLAM is a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. It has been demonstrated to show its effectiveness for simultaneous tracking of ego-motion and multiple objects. We release the 10 dynamic sequences rendered and simulated using SUNCG and CARLA dataset to facilitate research in our community.\nDataset Statistics The statistics are listed as follows. In total we have over 3000 frames with 60+ different dynamic instances.\nSequence Name # Frames # Dyn. Obj. # Landmarks Total Dist. (m) Download Link SUNCG-1-1 190 2 748 1.94 Google Drive SUNCG-1-2 250 2 2595 21.10 Google Drive SUNCG-2-1 300 3 381 6.03 Google Drive SUNCG-2-2 200 3 370 6.01 Google Drive SUNCG-3-1 200 5 554 3.61 Google Drive SUNCG-3-2 200 5 620 11.37 Google Drive CARLA-S1 200 5 2402 120.92 Google Drive CARLA-S2 200 8 4179 164.70 Google Drive CARLA-L1 750 14 13600 480.87 Google Drive CARLA-L2 600 17 10486 367.62 Google Drive Data Format We have zipped each sequence into individual packs with the name \u0026lt;Sequence Name\u0026gt;.tar.gz. The unzipped folder structure is as following:\n. // unzipped base folder ├── images │ ├── left │ │ └── %04d.png // {# Frames} images captured from left camera. │ └── right │ └── %04d.png // {# Frames} images captured from right camera. ├── landmarks │ ├── left │ │ └── %04d.txt // Detected features from left camera. │ └── right │ └── %04d.txt // Detected features from right camera. ├── pose │ └── %04d.txt // Trajectory of camera and moving instances. ├── shapes │ └── %d.pcd // Point cloud of the static scene and dynamic shapes. ├── instrinsic.txt // Stereo camera intrinsic. └── landmark_mapping.txt\t// Landmark to cluster id mapping. The format of each line of the feature text file is:\n\u0026lt;landmark id\u0026gt; \u0026lt;u\u0026gt; \u0026lt;v\u0026gt; For each trajectory file under pose/ directory, each line in each file represents the pose of each cluster except for the first line - that line represents the camera pose. You may notice that for some frames pose is still valid for invisible cluster - during our evaluation we eliminate these pose. The format for the pose is:\n\u0026lt;x\u0026gt; \u0026lt;y\u0026gt; \u0026lt;z\u0026gt; \u0026lt;qx\u0026gt; \u0026lt;qy\u0026gt; \u0026lt;qz\u0026gt; \u0026lt;qw\u0026gt; Translation Rotation which can be read simply using pyquaternion or Eigen library.\nThe point cloud file in the shapes/ folder is the ground-truth point cloud for both static scene and dynamic clusters. For the moving instances, by applying the point cloud with the transforms in pose/, you can get their absolute world coordinates in each frame.\nThe file intrinsic.txt has two $3 \\times 4$ projection matrices for the left and right camera, respectively. There is no rotation between cameras so stereo rectifying is not necessary.\nLastly, Each line in landmark_mapping.txt maps from landmark id to cluster id, this cluster id shares the same indices as the ground-truth point cloud and the line ordering of the trajectories files. Each line has the following format:\n\u0026lt;landmark id\u0026gt; \u0026lt;cluster id\u0026gt; Contact Email: huangjh.connect@outlook.com\nCitation:\n@inproceedings{huang2019clusterslam, title={ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation}, author={Huang, Jiahui and Yang, Sheng and Zhao, Zishuo and Lai, Yu-Kun and Hu, Shi-Min}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={5875--5884}, year={2019} } Change Log March, 2020: The 10 dynamic sequences used in our ICCV 2019 paper are released. Terms of Use The generated dataset as well as the annotations belong to the CSCG Group and are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e8a6ebe077f723d3e1f41043628e2e17","permalink":"https://huangjh-pub.github.io/page/clusterslam-dataset/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/page/clusterslam-dataset/","section":"page","summary":"Overview of the synthetic dataset Introduction ClusterSLAM is a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. It has been demonstrated to show its effectiveness for simultaneous tracking of ego-motion and multiple objects.","tags":null,"title":"ClusterSLAM Dataset","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4bb5d55b8d22b357234305377b2936f7","permalink":"https://huangjh-pub.github.io/page/clustervo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/page/clustervo/","section":"page","summary":"","tags":null,"title":"ClusterVO","type":"page"}]